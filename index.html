<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Penn Action by dreamdragon</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Penn Action</h1>
      <h2 class="project-tagline">Action Dataset with human body joints for all frames!</h2>
      <a href="https://github.com/dreamdragon/PennAction" class="btn">View on GitHub</a>
      <a href="https://github.com/dreamdragon/PennAction/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/dreamdragon/PennAction/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h2>
<a id="introducing-the-penn-action-dataset" class="anchor" href="#introducing-the-penn-action-dataset" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introducing the Penn Action Dataset</h2>

<p>Penn Action Dataset (University of Pennsylvania) contains 
2326 video sequences of 15 different actions and <strong>human 
joint annotations</strong> for each sequence. The dataset is 
available for <strong>download</strong> via the following link:</p>

<p><strong>Download:</strong> <a href="https://upenn.box.com/PennAction">https://upenn.box.com/PennAction</a></p>

<h2>
<a id="dataset-at-a-glance" class="anchor" href="#dataset-at-a-glance" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dataset At a Glance</h2>

<p><a href="https://www.youtube.com/watch?v=RjlFNGINFwE"><img src="http://www.seas.upenn.edu/%7Emenglong/PennAction/Action.png" alt=""></a></p>

<p><a href="https://www.youtube.com/watch?v=RjlFNGINFwE"><img src="http://www.seas.upenn.edu/%7Emenglong/PennAction/Keypoints.png" alt=""></a></p>

<h2>
<a id="reference" class="anchor" href="#reference" aria-hidden="true"><span class="octicon octicon-link"></span></a>Reference</h2>

<p>If you use our dataset, please cite the following paper:</p>

<pre><code>Weiyu Zhang, Menglong Zhu and Konstantinos Derpanis, "From Actemes to Action: 
A Strongly-supervised Representation for Detailed Action Understanding"
International Conference on Computer Vision (ICCV). Dec 2013.
</code></pre>

<h2>
<a id="dataset-content" class="anchor" href="#dataset-content" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dataset Content</h2>

<p>The dataset is organized in the following format:</p>

<pre><code>/frames  ( all image sequences )
   /0001 
      000001.jpg
      000002.jpg
      ...
   /0002
    ...
/labels  ( all annotations )
    0001.mat
    0002.mat
    ...
/tools   ( visualization scripts )
    visualize.m
    ...
</code></pre>

<p>The image frames are located in the /frames folder.
All frames are in RGB. The resolution of the frames 
are within the size of 640x480.</p>

<p>The annotations are in the /labels folder. The sequence annotations 
include class label, coarse viewpoint, human 
body joints (2D locations and visibility), 2D bounding boxes and training/testing 
label. Each annotation is a separate MATLAB .mat file under /labels.</p>

<p>An example annotation looks as follows in MATLAB:</p>

<pre><code>annotation = 

      action: 'tennis_serve'
        pose: 'back'
           x: [46x13 double]
           y: [46x13 double]
  visibility: [46x13 logical]
       train: 1
        bbox: [46x4 double]
  dimensions: [272 481 46]
     nframes: 46
</code></pre>

<h2>
<a id="list-of-actions" class="anchor" href="#list-of-actions" aria-hidden="true"><span class="octicon octicon-link"></span></a>List of Actions</h2>

<pre><code>baseball_pitch  clean_and_jerk  pull_ups  strumming_guitar  
baseball_swing  golf_swing      push_ups  tennis_forehand   
bench_press     jumping_jacks   sit_ups   tennis_serve
bowling         jump_rope       squats    
</code></pre>

<h2>
<a id="annotation-tools" class="anchor" href="#annotation-tools" aria-hidden="true"><span class="octicon octicon-link"></span></a>Annotation Tools</h2>

<p>The annotation tool used in creating this dataset is also available. 
Please refer to <a href="http://dreamdragon.github.io/vatic/">http://dreamdragon.github.io/vatic/</a> for more details.</p>

<h2>
<a id="contact" class="anchor" href="#contact" aria-hidden="true"><span class="octicon octicon-link"></span></a>Contact</h2>

<p>Please direct any questions regarding the dataset to</p>

<p>Menglong Zhu <a href="mailto:menglong@cis.upenn.edu">menglong@cis.upenn.edu</a></p>

<p><a href="http://cis.upenn.edu/%7Emenglong">http://cis.upenn.edu/~menglong</a></p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/dreamdragon/PennAction">Penn Action</a> is maintained by <a href="https://github.com/dreamdragon">dreamdragon</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

            <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
            document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
          </script>
          <script type="text/javascript">
            try {
              var pageTracker = _gat._getTracker("UA-49580934-1");
            pageTracker._trackPageview();
            } catch(err) {}
          </script>

  </body>
</html>
